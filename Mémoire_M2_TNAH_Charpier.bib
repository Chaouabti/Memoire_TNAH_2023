%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Marion Charpier at 2023-09-22 13:18:43 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@article{pedersoli_elastic_2015,
	author = {Pedersoli, Marco and Timofte, Radu and Tuytelaars, Tinne and Van Gool, Luc},
	date-added = {2023-09-22 13:18:33 +0200},
	date-modified = {2023-09-22 13:18:33 +0200},
	doi = {10.1007/s11263-014-0736-2},
	file = {Version accept{\'e}e:/Users/marioncharpier/Zotero/storage/SDYS3B46/Pedersoli et al. - 2015 - An Elastic Deformation Field Model for Object Dete.pdf:application/pdf},
	issn = {0920-5691, 1573-1405},
	journal = {International Journal of Computer Vision},
	language = {en},
	month = jan,
	number = {2},
	pages = {137--152},
	title = {An {Elastic} {Deformation} {Field} {Model} for {Object} {Detection} and {Tracking}},
	url = {http://link.springer.com/10.1007/s11263-014-0736-2},
	urldate = {2023-06-05},
	volume = {111},
	year = {2015},
	bdsk-url-1 = {http://link.springer.com/10.1007/s11263-014-0736-2},
	bdsk-url-2 = {https://doi.org/10.1007/s11263-014-0736-2}}

@incollection{bermes:hal-03991515,
	author = {Berm{\`e}s, Emmanuelle and Leclaire, C{\'e}line and Moreux, Jean-Philippe},
	booktitle = {{The Measurement of Images. Computational Approaches in the History and Theory of the Arts, sous la direction de Clarisse Bardiot et Emmanuel Ch{\^a}teau-Dutier}},
	date-added = {2023-09-22 13:13:28 +0200},
	date-modified = {2023-09-22 13:13:28 +0200},
	hal_id = {hal-03991515},
	hal_version = {v1},
	keywords = {Artificial intelligence ; Image mining ; Digital libraries ; Computer vision ; Intelligence artificielle ; Fouille d'images ; Biblioth{\`e}ques num{\'e}riques ; Vision par ordinateur},
	note = {Cet article est propos{\'e} en pr{\'e}publication avant sa diffusion dans l'ouvrage ''The Measurement of Images. Computational Approaches in the History and Theory of the Arts'' sous la direction de Clarisse Bardiot et Emmanuel Ch{\^a}teau-Dutier, Villeneuve d'Ascq, Presses universitaires du Septentrion, collection Humanit{\'e}s num{\'e}riques et science ouverte, {\`a} para{\^\i}tre en 2023},
	pdf = {https://hal.science/hal-03991515/file/Article%20DH%20Nord_2022_BnF_Bermes%20Leclaire%20Moreux.pdf},
	publisher = {{Presses universitaires du Septentrion}},
	series = {Humanit{\'e}s num{\'e}riques et science ouverte},
	title = {{L'image comme particule {\'e}l{\'e}mentaire, ou les pr{\'e}misses d'un changement d'{\'e}chelle {\`a} la BnF}},
	url = {https://hal.science/hal-03991515},
	year = {2023},
	bdsk-url-1 = {https://hal.science/hal-03991515}}

@misc{kirillov2023segment,
	archiveprefix = {arXiv},
	author = {Alexander Kirillov and Eric Mintun and Nikhila Ravi and Hanzi Mao and Chloe Rolland and Laura Gustafson and Tete Xiao and Spencer Whitehead and Alexander C. Berg and Wan-Yen Lo and Piotr Doll{\'a}r and Ross Girshick},
	date-added = {2023-09-22 13:13:28 +0200},
	date-modified = {2023-09-22 13:13:28 +0200},
	eprint = {2304.02643},
	primaryclass = {cs.CV},
	title = {Segment Anything},
	year = {2023}}

@article{Lang2021TransformingII,
	author = {Sabine Lang and Bj{\"o}rn Ommer},
	date-added = {2023-09-22 13:13:28 +0200},
	date-modified = {2023-09-22 13:13:28 +0200},
	journal = {Digit. Humanit. Q.},
	title = {Transforming Information Into Knowledge: How Computational Methods Reshape Art History},
	url = {https://api.semanticscholar.org/CorpusID:243845008},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://api.semanticscholar.org/CorpusID:243845008}}

@software{yolov8_ultralytics,
	author = {Glenn Jocher and Ayush Chaurasia and Jing Qiu},
	date-added = {2023-09-22 13:13:28 +0200},
	date-modified = {2023-09-22 13:13:28 +0200},
	license = {AGPL-3.0},
	orcid = {0000-0001-5950-6979, 0000-0002-7603-6750, 0000-0003-3783-7069},
	title = {Ultralytics YOLOv8},
	url = {https://github.com/ultralytics/ultralytics},
	version = {8.0.0},
	year = {2023},
	bdsk-url-1 = {https://github.com/ultralytics/ultralytics}}

@article{boillet_horae_2020,
	abstract = {We introduce in this paper a new dataset of annotated pages from books of hours, a type of handwritten prayer books owned and used by rich lay people in the late middle ages. The dataset was created for conducting historical research on the evolution of the religious mindset in Europe at this period since the book of hours represent one of the major sources of information thanks both to their rich illustrations and the different types of religious sources they contain. We first describe how the corpus was collected and manually annotated then present the evaluation of a state-of-the-art system for text line detection and for zone detection and typing. The corpus is freely available for research.},
	author = {Boillet, M{\'e}lodie and Bonhomme, Marie-Laurence and Stutzmann, Dominique and Kermorvant, Christopher},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.2012.00351},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	note = {Publisher: arXiv Version Number: 1},
	shorttitle = {{HORAE}},
	title = {{HORAE}: an annotated dataset of books of hours},
	url = {https://arxiv.org/abs/2012.00351},
	urldate = {2023-06-29},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2012.00351},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2012.00351}}

@article{yang_automatic_2017,
	abstract = {We propose three automatic algorithms for analyzing digitized medieval manuscripts,
              text block computation
              ,
              text line segmentation
              , and
              special component extraction
              , by taking advantage of previous clustering algorithms and a template-matching technique. These three methods are completely automatic, so no user intervention or input is required to make them work. Moreover, they are all per-page based; that is, unlike some prior methods---that need a set of pages from the same manuscript for training purposes---they are able to analyze a single page without requiring any additional pages for input, eliminating the need for training on additional pages with similar layout. We extensively evaluated the algorithms on 1,771 images of pages of six different publicly available historical manuscripts, which differ significantly from each other in terms of layout structure, acquisition resolution, writing style, and so on. The experimental results indicate that they are able to achieve very satisfactory performance, that is, the average precision and recall values obtained by the
              text block computation method
              can reach as high as 98\% and 99\%, respectively.},
	author = {Yang, Ying and Pintus, Ruggero and Gobbetti, Enrico and Rushmeier, Holly},
	doi = {10.1145/2996469},
	issn = {1556-4673, 1556-4711},
	journal = {Journal on Computing and Cultural Heritage},
	language = {en},
	month = apr,
	number = {2},
	pages = {1--22},
	title = {Automatic {Single} {Page}-{Based} {Algorithms} for {Medieval} {Manuscript} {Analysis}},
	url = {https://dl.acm.org/doi/10.1145/2996469},
	urldate = {2023-08-19},
	volume = {10},
	year = {2017},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/2996469},
	bdsk-url-2 = {https://doi.org/10.1145/2996469}}

@article{sartini_icon_2023,
	abstract = {In this work, we introduce ICON, an ontology that models artistic interpretations of artworks' subject matter (i.e., iconographies) and meanings (i.e., symbols, iconological aspects). Developed by conceptualizing authoritative knowledge and notions taken from Panofsky's levels of interpretation theory, ICON ontology focuses on the granularity of interpretations. It can be used to describe an interpretation of an artwork from the pre-iconographical, icongraphical, and iconological levels. Its main classes have been aligned to ontologies that come from the domains of cultural descriptions (ArCo, CIDOC-CRM, VIR), semiotics (DOLCE), bibliometrics (CITO), and symbolism (Simulation Ontology), to grant a robust schema that can be extendable using additional classes and properties coming from these ontologies. The ontology was evaluated through competency questions that range from simple recognition on a specific level of interpretation to complex scenarios. Data written using this model was compared to state-of-the-art ontologies and schemas to both highlight the current lack of a domain-specific ontology on art interpretation and show how our work fills some of the current gaps. The ontology is openly available and compliant with FAIR principles. With our ontology, we hope to encourage digital art historians working for cultural institutions in making more detailed linked open data about the content of their artifacts, to exploit the full potential of Semantic Web in linking artworks through not only subjects and common metadata but also specific symbolic interpretations, intrinsic meanings, and the motifs through which their subjects are represented. Additionally, by basing our work on theories made by different art history scholars in the last century, we make sure that their knowledge and studies will not be lost in the transition to the digital, linked open data era.},
	author = {Sartini, Bruno and Baroncini, Sofia and Van Erp, Marieke and Tomasi, Francesca and Gangemi, Aldo},
	doi = {10.1145/3594724},
	file = {Texte int{\'e}gral:/Users/marioncharpier/Zotero/storage/KFP2PRSN/Sartini et al. - 2023 - ICON An Ontology for Comprehensive Artistic Inter.pdf:application/pdf},
	issn = {1556-4673, 1556-4711},
	journal = {Journal on Computing and Cultural Heritage},
	language = {en},
	month = sep,
	number = {3},
	pages = {1--38},
	shorttitle = {{ICON}},
	title = {{ICON}: {An} {Ontology} for {Comprehensive} {Artistic} {Interpretations}},
	url = {https://dl.acm.org/doi/10.1145/3594724},
	urldate = {2023-08-19},
	volume = {16},
	year = {2023},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3594724},
	bdsk-url-2 = {https://doi.org/10.1145/3594724}}

@article{baroncini_modelling_2021,
	abstract = {Iconology is a branch of art history that investigates the meaning of artworks in relation to their social and cultural background. Nowadays, several interdisciplinary research fields leverage theoretical frameworks close to iconology to pursue quantitative Art History with data science methods and Semantic Web technologies. However, while Iconographic studies have been recently addressed in ontologies, a complete description of aspects relevant to iconological studies is still missing. In this article, we present a preliminary study on eleven case studies selected from the literature and we envision new terms for extending existing ontologies. We validate new terms according to a common evaluation method and we discuss our results in the light of the opportunities that such an extended ontology would arise in the community of Digital Art History.},
	annote = {Other
16 pages, 7 figures},
	author = {Baroncini, S. and Daquino, M. and Tomasi, F.},
	copyright = {Creative Commons Attribution 4.0 International},
	doi = {10.48550/ARXIV.2106.12967},
	keywords = {FOS: Computer and information sciences, Artificial Intelligence (cs.AI), Digital Libraries (cs.DL), J.5; I.2.4},
	note = {Publisher: arXiv Version Number: 1},
	title = {Modelling {Art} {Interpretation} and {Meaning}. {A} {Data} {Model} for {Describing} {Iconology} and {Iconography}},
	url = {https://arxiv.org/abs/2106.12967},
	urldate = {2023-08-19},
	year = {2021},
	bdsk-url-1 = {https://arxiv.org/abs/2106.12967},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2106.12967}}

@article{carboni_ontological_2019,
	abstract = {The perception of our heritage is based on sign-functions, which relate visual representations to cognitive types, allowing us to make perceptual judgements over physical objects. The recording of these types of assertions is paramount for the comprehension and analysis of our heritage. The article investigates a theoretical framework for the organization of information related to visual works on the basis of the identity and symbolic value of their single constituent elements. The framework developed is then used as a driver for the grounding of a new ontology called VIR (Visual Representation), constructed as an extension of CIDOC-CRM (CIDOC Conceptual Reference Model). VIR sustains the recording of statements about the different structural units and relationships of a visual representation, differentiating between object and interpretative act. The result, tested with data describing Byzantine and Renaissance artworks, presents solutions for describing symbols and meanings of iconographical objects, providing new clustering methods in relation to their constitutive elements, subjects or interpretations.},
	author = {{Carboni} and {Luca}},
	doi = {10.3390/heritage2020078},
	file = {Texte int{\'e}gral:/Users/marioncharpier/Zotero/storage/CLN7ULP6/Carboni et Luca - 2019 - An Ontological Approach to the Description of Visu.pdf:application/pdf},
	issn = {2571-9408},
	journal = {Heritage},
	language = {en},
	month = apr,
	number = {2},
	pages = {1191--1210},
	title = {An {Ontological} {Approach} to the {Description} of {Visual} and {Iconographical} {Representations}},
	url = {https://www.mdpi.com/2571-9408/2/2/78},
	urldate = {2023-08-19},
	volume = {2},
	year = {2019},
	bdsk-url-1 = {https://www.mdpi.com/2571-9408/2/2/78},
	bdsk-url-2 = {https://doi.org/10.3390/heritage2020078}}

@article{moreux_recherche_2019,
	abstract = {Si historiquement, les biblioth{\`e}ques num{\'e}riques patrimoniales furent d'abord aliment{\'e}es par des images, elles profit{\`e}rent rapidement de la technologie OCR pour indexer les collections imprim{\'e}es afin d'am{\'e}liorer le service de recherche d'information offert aux utilisateurs. Mais l'acc{\`e}s aux ressources iconographiques n'a pas connu les m{\^e}mes progr{\`e}s et ces derni{\`e}res demeurent dans l'ombre : indexation manuelle lacunaire, h{\'e}t{\'e}rog{\`e}ne et impossible {\`a} g{\'e}n{\'e}raliser ; silos par genre documentaire ; recherche dans le contenu des images encore peu op{\'e}rationnelle sur les collections patrimoniales. Aujourd'hui, il serait pourtant possible de mieux valoriser ces ressources en exploitant les {\'e}normes volumes d'OCR produits durant les deux derni{\`e}res d{\'e}cennies (tant comme descripteur textuel que pour l'identification automatique des illustrations des imprim{\'e}s), en profitant de la maturit{\'e} des techniques d'intelligence artificielle (en particulier l'apprentissage profond ou
              deep learning
              ), pour mettre ainsi en valeur ces gravures, dessins, photographies, cartes, etc., pour leur valeur propre, mais aussi comme point d'entr{\'e}e dans les collections, en favorisant d{\'e}couverte et rebond.
            
            Cet article d{\'e}crit une approche ETL (extract-transform-load) appliqu{\'e}e aux images d'une biblioth{\`e}que num{\'e}rique {\`a} vocation encyclop{\'e}dique : identifier et extraire l'iconographie partout o{\`u} elle se trouve (dans les collections d'images, mais aussi dans les imprim{\'e}s) ; transformer, harmoniser et enrichir ses m{\'e}tadonn{\'e}es descriptives gr{\^a}ce {\`a} l'IA ; int{\'e}grer ces donn{\'e}es dans une application web d{\'e}di{\'e}e {\`a} la recherche iconographique. Cette approche est qualifi{\'e}e de pragmatique {\`a} double titre, puisqu'il s'agit de valoriser des ressources num{\'e}riques existantes tout en mettant {\`a} profit les acquis de l'IA.
          , 
            
              If historically, heritage digital libraries were initially made up of images, they rapidly benefited from the optical character recognition (OCR) technology to index print collections and improve reference services for users. However, access to iconographic resources has not experienced the same progression, remaining somewhat difficult to access. Manual indexation is not very efficient, it is varied and impossible to apply uniformly. Searching the content of an image is not as effective with heritage collections. Today, it is possible to improve the use of these resources by exploiting large volumes of OCR produced over the past two decades (both the textual descriptors as well as the automatic identification of the illustrations in the printed documents) and to take advantage of proven artificial intelligence techniques, especially deep learning. In doing so, it will showcase engravings, drawings, photographs, maps, etc. as such but also the point of entry to the collections by improving discovery and connections.
            
            
              This article describes an ETL (extract-transform-load) approach as it applies to the images in a digital library with an encyclopedic vocation. There are three components: 1) identify and extract the iconography wherever it is found, either in images or in the printed documents, 2) transform, harmonise and enrich the descriptive metadata with the help of artificial intelligence, and 3) incorporate this data into a web application dedicated to iconographic research. This is a two-pronged approach because it highlights existing digital resources and takes advantage of the benefits of artificial intelligence.},
	author = {Moreux, Jean-Philippe},
	doi = {10.7202/1063786ar},
	issn = {2291-8949, 0315-2340},
	journal = {Documentation et biblioth{\`e}ques},
	month = sep,
	number = {2},
	pages = {5--27},
	title = {Recherche d'images dans les biblioth{\`e}ques num{\'e}riques patrimoniales et exp{\'e}rimentation de techniques d'apprentissage profond},
	url = {http://id.erudit.org/iderudit/1063786ar},
	urldate = {2023-08-23},
	volume = {65},
	year = {2019},
	bdsk-url-1 = {http://id.erudit.org/iderudit/1063786ar},
	bdsk-url-2 = {https://doi.org/10.7202/1063786ar}}

@phdthesis{thiele_capsules_2019,
	address = {M{\"u}nster},
	author = {Thiele, Sebastian},
	language = {German},
	month = sep,
	school = {Westf{\"a}lische Wilhelms-Universit{\"a}t M{\"u}nster Fachbereich Mathematik und Informatik Institut f{\"u}r Informatik},
	title = {Capsules {Only} {Look} {Once}: {Ein} {Vergleich} von {Capsule} {Networks} mit {State}-of-the-{Art} {Deep} {Learning} {Architekturen} in einer neuartigen {Wappen}-{Bilddom{\"a}ne}},
	type = {Master of {Science}},
	year = {2019}}

@misc{stutzmann_texte_2021,
	address = {Rennes},
	author = {Stutzmann, Dominique and Chevalier, Louis},
	month = may,
	title = {Texte et image : les livres d'heures manuscrit vus par l'intelligance artificielle},
	year = {2021}}

@article{everingham_pascal_2010,
	author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
	doi = {10.1007/s11263-009-0275-4},
	file = {Version soumise:/Users/marioncharpier/Zotero/storage/H2NLTDS7/Everingham et al. - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf:application/pdf},
	issn = {0920-5691, 1573-1405},
	journal = {International Journal of Computer Vision},
	language = {en},
	month = jun,
	number = {2},
	pages = {303--338},
	title = {The {Pascal} {Visual} {Object} {Classes} ({VOC}) {Challenge}},
	url = {http://link.springer.com/10.1007/s11263-009-0275-4},
	urldate = {2023-08-28},
	volume = {88},
	year = {2010},
	bdsk-url-1 = {http://link.springer.com/10.1007/s11263-009-0275-4},
	bdsk-url-2 = {https://doi.org/10.1007/s11263-009-0275-4}}

@article{le_roy_ladurie_fin_1968,
	author = {Le Roy Ladurie, Emmanuel},
	journal = {Le Nouvel Observateur},
	month = may,
	title = {La fin des {\'e}rudits},
	year = {1968}}

@misc{hiltmann_digital-history-berlindigital-methods--practice_2021,
	abstract = {This release publishes source code and research data pertaining to the paper "Torsten Hiltmann, Jan Keupp, Melanie Althage, Philipp Schneider: Digital Methods in Practice. The Epistemological Implications of Applying Text Re-Use Analysis to the Bloody Accounts of the Conquest of Jerusalem (1099), in: Geschichte und Gesellschaft 47, 2021"},
	author = {Hiltmann, Torsten and Keupp, Jan and Althage, Melanie and Schneider, Philipp},
	copyright = {Open Access},
	doi = {10.5281/ZENODO.4719838},
	month = apr,
	publisher = {Zenodo},
	shorttitle = {Digital-{History}-{Berlin}/digital-methods-in-practice},
	title = {Digital-{History}-{Berlin}/digital-methods-in-practice: {Publication} of source code and data},
	url = {https://zenodo.org/record/4719838},
	urldate = {2023-08-30},
	year = {2021},
	bdsk-url-1 = {https://zenodo.org/record/4719838},
	bdsk-url-2 = {https://doi.org/10.5281/ZENODO.4719838}}

@book{szeliski_computer_2022,
	address = {Cham},
	author = {Szeliski, Richard},
	edition = {Second edition},
	file = {Table of Contents PDF:/Users/marioncharpier/Zotero/storage/IY96N9GZ/Szeliski - 2022 - Computer vision algorithms and applications.pdf:application/pdf},
	isbn = {978-3-030-34371-2},
	language = {eng},
	publisher = {Springer},
	series = {Texts in computer science},
	shorttitle = {Computer vision},
	title = {Computer vision: algorithms and applications},
	year = {2022}}

@article{gruber_translation_1993,
	author = {Gruber, Thomas R.},
	doi = {10.1006/knac.1993.1008},
	file = {Version soumise:/Users/marioncharpier/Zotero/storage/6DWDYPUR/Gruber - 1993 - A translation approach to portable ontology specif.pdf:application/pdf},
	issn = {10428143},
	journal = {Knowledge Acquisition},
	language = {en},
	month = jun,
	number = {2},
	pages = {199--220},
	title = {A translation approach to portable ontology specifications},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1042814383710083},
	urldate = {2023-08-31},
	volume = {5},
	year = {1993},
	bdsk-url-1 = {https://linkinghub.elsevier.com/retrieve/pii/S1042814383710083},
	bdsk-url-2 = {https://doi.org/10.1006/knac.1993.1008}}

@article{bleier_digitale_2019,
	author = {Bleier, Roman and Fischer, Franz and Hiltmann, Torsten and Viehhauser, Gabriel and Vogeler, Georg},
	doi = {10.1515/mial-2019-0001},
	issn = {2196-6869, 0949-0345},
	journal = {Das Mittelalter},
	language = {en},
	month = jul,
	number = {1},
	pages = {1--12},
	title = {Digitale {Medi{\"a}vistik} und der deutschsprachige {Raum}},
	url = {https://www.degruyter.com/document/doi/10.1515/mial-2019-0001/html},
	urldate = {2023-09-01},
	volume = {24},
	year = {2019},
	bdsk-url-1 = {https://www.degruyter.com/document/doi/10.1515/mial-2019-0001/html},
	bdsk-url-2 = {https://doi.org/10.1515/mial-2019-0001}}

@article{hiltmann_digital_2021,
	author = {Hiltmann, Torsten and Keupp, Jan and Althage, Melanie and Schneider, Philipp},
	doi = {10.13109/gege.2021.47.1.122},
	file = {Texte int{\'e}gral:/Users/marioncharpier/Zotero/storage/PRQDTWJ5/Hiltmann et al. - 2021 - Digital Methods in Practice The Epistemological I.pdf:application/pdf},
	issn = {0340-613X, 2196-9000},
	journal = {Geschichte und Gesellschaft},
	language = {de},
	month = jun,
	number = {1},
	pages = {122--156},
	shorttitle = {Digital {Methods} in {Practice}},
	title = {Digital {Methods} in {Practice}: {The} {Epistemological} {Implications} of {Applying} {Text} {Re}-{Use} {Analysis} to the {Bloody} {Accounts} of the {Conquest} of {Jerusalem} (1099)},
	url = {https://www.vr-elibrary.de/doi/10.13109/gege.2021.47.1.122},
	urldate = {2023-09-01},
	volume = {47},
	year = {2021},
	bdsk-url-1 = {https://www.vr-elibrary.de/doi/10.13109/gege.2021.47.1.122},
	bdsk-url-2 = {https://doi.org/10.13109/gege.2021.47.1.122}}

@misc{hiltmann_heraldry_2016,
	author = {Hiltmann, Torsten},
	journal = {Heraldica Nova: Medieval and Early Modern Heraldry from the Perspective of Cultural History (a Hypotheses.org blog)},
	language = {eng},
	month = may,
	title = {Heraldry as a {Systematic} and {International} {Language}? {About} the {Limitations} of {Blazonry} in {Describing} {Coats} of {Arms}},
	url = {https://heraldica.hypotheses.org/4623},
	urldate = {2023-08-12},
	year = {2016},
	bdsk-url-1 = {https://heraldica.hypotheses.org/4623}}

@misc{wang_yolov7_2022,
	abstract = {YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8\% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6 object detector (56 FPS V100, 55.9\% AP) outperforms both transformer-based detector SWIN-L Cascade-Mask R-CNN (9.2 FPS A100, 53.9\% AP) by 509\% in speed and 2\% in accuracy, and convolutional-based detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2\% AP) by 551\% in speed and 0.7\% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. Source code is released in https://github.com/WongKinYiu/yolov7.},
	author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
	file = {arXiv Fulltext PDF:/Users/marioncharpier/Zotero/storage/FJ7V6NDU/Wang et al. - 2022 - YOLOv7 Trainable bag-of-freebies sets new state-o.pdf:application/pdf;arXiv.org Snapshot:/Users/marioncharpier/Zotero/storage/JBID8GLL/2207.html:text/html},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	month = jul,
	note = {arXiv:2207.02696 [cs]},
	publisher = {arXiv},
	shorttitle = {{YOLOv7}},
	title = {{YOLOv7}: {Trainable} bag-of-freebies sets new state-of-the-art for real-time object detectors},
	url = {http://arxiv.org/abs/2207.02696},
	urldate = {2023-09-01},
	year = {2022},
	bdsk-url-1 = {http://arxiv.org/abs/2207.02696}}

@incollection{hutchison_dataset_2006,
	address = {Berlin, Heidelberg},
	author = {Ponce, J. and Berg, T. L. and Everingham, M. and Forsyth, D. A. and Hebert, M. and Lazebnik, S. and Marszalek, M. and Schmid, C. and Russell, B. C. and Torralba, A. and Williams, C. K. I. and Zhang, J. and Zisserman, A.},
	booktitle = {Toward {Category}-{Level} {Object} {Recognition}},
	doi = {10.1007/11957959_2},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Ponce, Jean and Hebert, Martial and Schmid, Cordelia and Zisserman, Andrew},
	isbn = {978-3-540-68794-8 978-3-540-68795-5},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {29--48},
	publisher = {Springer Berlin Heidelberg},
	title = {Dataset {Issues} in {Object} {Recognition}},
	url = {http://link.springer.com/10.1007/11957959_2},
	urldate = {2023-09-01},
	volume = {4170},
	year = {2006},
	bdsk-url-1 = {http://link.springer.com/10.1007/11957959_2},
	bdsk-url-2 = {https://doi.org/10.1007/11957959_2}}

@article{kaoua_image_2021,
	abstract = {Illustrations are an essential transmission instrument. For an historian, the first step in studying their evolution in a corpus of similar manuscripts is to identify which ones correspond to each other. This image collation task is daunting for manuscripts separated by many lost copies, spreading over centuries, which might have been completely re-organized and greatly modified to adapt to novel knowledge or belief and include hundreds of illustrations. Our contributions in this paper are threefold. First, we introduce the task of illustration collation and a large annotated public dataset to evaluate solutions, including 6 manuscripts of 2 different texts with more than 2 000 illustrations and 1 200 annotated correspondences. Second, we analyze state of the art similarity measures for this task and show that they succeed in simple cases but struggle for large manuscripts when the illustrations have undergone very significant changes and are discriminated only by fine details. Finally, we show clear evidence that significant performance boosts can be expected by exploiting cycle-consistent correspondences. Our code and data are available on http://imagine.enpc.fr/{\textasciitilde}shenx/ImageCollation.},
	annote = {Other
accepted to ICDAR 2021},
	author = {Kaoua, Ryad and Shen, Xi and Durr, Alexandra and Lazaris, Stavros and Picard, David and Aubry, Mathieu},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	doi = {10.48550/ARXIV.2108.08109},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	note = {Publisher: arXiv Version Number: 1},
	shorttitle = {Image {Collation}},
	title = {Image {Collation}: {Matching} illustrations in manuscripts},
	url = {https://arxiv.org/abs/2108.08109},
	urldate = {2023-09-03},
	year = {2021},
	bdsk-url-1 = {https://arxiv.org/abs/2108.08109},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2108.08109}}

@article{redmon_you_2015,
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1506.02640},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	note = {Publisher: arXiv Version Number: 5},
	shorttitle = {You {Only} {Look} {Once}},
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	url = {https://arxiv.org/abs/1506.02640},
	urldate = {2023-09-03},
	year = {2015},
	bdsk-url-1 = {https://arxiv.org/abs/1506.02640},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1506.02640}}

@book{alpaydin_introduction_2014,
	address = {Cambridge, Massachusetts},
	annote = {Introduction -- Supervised learning -- Bayesian decision theory -- Parametric methods -- Multivariate methods -- Dimensionality reduction -- Clustering -- Nonparametric methods -- Decision trees -- Linear discrimination -- Multilayer perceptrons -- Local models -- Kernel machines -- Graphical models -- Hidden markov models -- Bayesian estimation -- Combining multiple learners -- Reinforcement learning -- Design and analysis of machine learning experiments},
	author = {Alpaydin, Ethem},
	edition = {Third edition},
	isbn = {978-0-262-02818-9},
	keywords = {Machine learning},
	publisher = {The MIT Press},
	series = {Adaptive computation and machine learning},
	title = {Introduction to machine learning},
	year = {2014}}

@article{girshick_rich_2013,
	abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\textasciitilde}rbg/rcnn.},
	annote = {Other
Extended version of our CVPR 2014 paper; latest update (v5) includes results using deeper networks (see Appendix G. Changelog)},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1311.2524},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	note = {Publisher: arXiv Version Number: 5},
	title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
	url = {https://arxiv.org/abs/1311.2524},
	urldate = {2023-08-15},
	year = {2013},
	bdsk-url-1 = {https://arxiv.org/abs/1311.2524},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1311.2524}}

@article{jiao_survey_2019,
	abstract = {Object detection is one of the most important and challenging branches of computer vision, which has been widely applied in peoples life, such as monitoring security, autonomous driving and so on, with the purpose of locating instances of semantic objects of a certain class. With the rapid development of deep learning networks for detection tasks, the performance of object detectors has been greatly improved. In order to understand the main development status of object detection pipeline, thoroughly and deeply, in this survey, we first analyze the methods of existing typical detection models and describe the benchmark datasets. Afterwards and primarily, we provide a comprehensive overview of a variety of object detection methods in a systematic manner, covering the one-stage and two-stage detectors. Moreover, we list the traditional and new applications. Some representative branches of object detection are analyzed as well. Finally, we discuss the architecture of exploiting these object detection methods to build an effective and efficient system and point out a set of development trends to better follow the state-of-the-art algorithms and further research.},
	annote = {Other
30 pages,12 figures},
	author = {Jiao, Licheng and Zhang, Fan and Liu, Fang and Yang, Shuyuan and Li, Lingling and Feng, Zhixi and Qu, Rong},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1907.09408},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	note = {Publisher: arXiv Version Number: 2},
	title = {A {Survey} of {Deep} {Learning}-based {Object} {Detection}},
	url = {https://arxiv.org/abs/1907.09408},
	urldate = {2023-08-05},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1907.09408},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1907.09408}}

@article{minderer_simple_2022,
	abstract = {Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models are available on GitHub.},
	annote = {Other
ECCV 2022 camera-ready version},
	author = {Minderer, Matthias and Gritsenko, Alexey and Stone, Austin and Neumann, Maxim and Weissenborn, Dirk and Dosovitskiy, Alexey and Mahendran, Aravindh and Arnab, Anurag and Dehghani, Mostafa and Shen, Zhuoran and Wang, Xiao and Zhai, Xiaohua and Kipf, Thomas and Houlsby, Neil},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.2205.06230},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	note = {Publisher: arXiv Version Number: 2},
	title = {Simple {Open}-{Vocabulary} {Object} {Detection} with {Vision} {Transformers}},
	url = {https://arxiv.org/abs/2205.06230},
	urldate = {2023-08-16},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2205.06230},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2205.06230}}

@article{ibrahim_few-shot_2022,
	abstract = {Detecting objects with a small representation in images is a challenging task, especially when the style of the images is very different from recent photos, which is the case for cultural heritage datasets. This problem is commonly known as few-shot object detection and is still a new field of research. This article presents a simple and effective method for black box few-shot object detection that works with all the current state-of-the-art object detection models. We also present a new dataset called MMSD for medieval musicological studies that contains five classes and 693 samples, manually annotated by a group of musicology experts. Due to the significant diversity of styles and considerable disparities between the artistic representations of the objects, our dataset is more challenging than the current standards. We evaluate our method on YOLOv4 (m/s), (Mask/Faster) RCNN, and ViT/Swin-t. We present two methods of benchmarking these models based on the overall data size and the worst-case scenario for object detection. The experimental results show that our method always improves object detector results compared to traditional transfer learning, regardless of the underlying architecture.},
	author = {Ibrahim, Bekkouch Imad Eddine and Eyharabide, Victoria and Le Page, Val{\'e}rie and Billiet, Fr{\'e}d{\'e}ric},
	doi = {10.3390/jimaging8020018},
	file = {Texte int{\'e}gral:/Users/marioncharpier/Zotero/storage/KZ55SCQT/Ibrahim et al. - 2022 - Few-Shot Object Detection Application to Medieval.pdf:application/pdf},
	issn = {2313-433X},
	journal = {Journal of Imaging},
	language = {en},
	month = jan,
	number = {2},
	pages = {18},
	shorttitle = {Few-{Shot} {Object} {Detection}},
	title = {Few-{Shot} {Object} {Detection}: {Application} to {Medieval} {Musicological} {Studies}},
	url = {https://www.mdpi.com/2313-433X/8/2/18},
	urldate = {2023-08-19},
	volume = {8},
	year = {2022},
	bdsk-url-1 = {https://www.mdpi.com/2313-433X/8/2/18},
	bdsk-url-2 = {https://doi.org/10.3390/jimaging8020018}}

@article{ufer_large-scale_2021,
	abstract = {Finding objects and motifs across artworks is of great importance for art history as it helps to understand individual works and analyze relations between them. The advent of digitization has produced extensive digital art collections with many research opportunities. However, manual approaches are inadequate to handle this amount of data, and it requires appropriate computer-based methods to analyze them. This article presents a visual search algorithm and user interface to support art historians to find objects and motifs in extensive datasets. Artistic image collections are subject to significant domain shifts induced by large variations in styles, artistic media, and materials. This poses new challenges to most computer vision models which are trained on photographs. To alleviate this problem, we introduce a multi-style feature aggregation that projects images into the same distribution, leading to more accurate and style-invariant search results. Our retrieval system is based on a voting procedure combined with fast nearest-neighbor search and enables finding and localizing motifs within an extensive image collection in seconds. The presented approach significantly improves the state-of-the-art in terms of accuracy and search time on various datasets and applies to large and inhomogeneous collections. In addition to the search algorithm, we introduce a user interface that allows art historians to apply our algorithm in practice. The interface enables users to search for single regions, multiple regions regarding different connection types and holds an interactive feedback system to improve retrieval results further. With our methodological contribution and easy-to-use user interface, this work manifests further progress towards a computer-based analysis of visual art.},
	author = {Ufer, Nikolai and Simon, Max and Lang, Sabine and Ommer, Bj{\"o}rn},
	doi = {10.1371/journal.pone.0259718},
	editor = {Oliva, Diego},
	file = {Texte int{\'e}gral:/Users/marioncharpier/Zotero/storage/QUGS2NAP/Ufer et al. - 2021 - Large-scale interactive retrieval in art collectio.pdf:application/pdf},
	issn = {1932-6203},
	journal = {PLOS ONE},
	language = {en},
	month = nov,
	number = {11},
	pages = {e0259718},
	title = {Large-scale interactive retrieval in art collections using multi-style feature aggregation},
	url = {https://dx.plos.org/10.1371/journal.pone.0259718},
	urldate = {2023-08-19},
	volume = {16},
	year = {2021},
	bdsk-url-1 = {https://dx.plos.org/10.1371/journal.pone.0259718},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pone.0259718}}

@article{aouinti_illumination_2022,
	abstract = {Illuminated manuscripts are essential iconographic sources for medieval studies. With the massive adoption of IIIF, old and new digital collections of manuscripts are accessible online and provide interoperable image data. However, finding illuminations within the manuscripts' pages is increasingly time consuming. This article proposes an approach based on machine learning and transfer learning that browses IIIF manuscript pages and detects the illuminated ones. To evaluate our approach, a group of domain experts created a new dataset of manually annotated IIIF manuscripts. The preliminary results show that our algorithm detects the main illuminated pages in a manuscript, thus reducing experts' search time.},
	author = {Aouinti, Fouad and Eyharabide, Victoria and Fresquet, Xavier and Billiet, Fr{\'e}d{\'e}ric},
	doi = {10.16995/dm.8073},
	file = {Texte int{\'e}gral:/Users/marioncharpier/Zotero/storage/X6ZWCUXG/Aouinti et al. - 2022 - Illumination Detection in IIIF Medieval Manuscript.pdf:application/pdf},
	issn = {1715-0736},
	journal = {Digital Medievalist},
	month = aug,
	number = {1},
	title = {Illumination {Detection} in {IIIF} {Medieval} {Manuscripts} {Using} {Deep} {Learning}},
	url = {https://journal.digitalmedievalist.org/article/id/8073/},
	urldate = {2023-08-19},
	volume = {15},
	year = {2022},
	bdsk-url-1 = {https://journal.digitalmedievalist.org/article/id/8073/},
	bdsk-url-2 = {https://doi.org/10.16995/dm.8073}}

@inproceedings{schlecht_detecting_2011,
	address = {Brussels, Belgium},
	author = {Schlecht, Joseph and Carque, Bernd and Ommer, Bjorn},
	booktitle = {2011 18th {IEEE} {International} {Conference} on {Image} {Processing}},
	doi = {10.1109/ICIP.2011.6115669},
	isbn = {978-1-4577-1303-3 978-1-4577-1304-0 978-1-4577-1302-6},
	month = sep,
	pages = {1285--1288},
	publisher = {IEEE},
	title = {Detecting gestures in medieval images},
	url = {http://ieeexplore.ieee.org/document/6115669/},
	urldate = {2023-08-19},
	year = {2011},
	bdsk-url-1 = {http://ieeexplore.ieee.org/document/6115669/},
	bdsk-url-2 = {https://doi.org/10.1109/ICIP.2011.6115669}}

@inproceedings{mehri_hba_2017,
	address = {Kyoto Japan},
	author = {Mehri, Maroua and H{\'e}roux, Pierre and Mullot, R{\'e}my and Moreux, Jean-Philippe and Co{\"u}asnon, Bertrand and Barrett, Bill},
	booktitle = {Proceedings of the 4th {International} {Workshop} on {Historical} {Document} {Imaging} and {Processing}},
	doi = {10.1145/3151509.3151528},
	file = {Version soumise:/Users/marioncharpier/Zotero/storage/44MD8W7P/Mehri et al. - 2017 - HBA 1.0 A Pixel-based Annotated Dataset for Histo.pdf:application/pdf},
	isbn = {978-1-4503-5390-8},
	language = {en},
	month = nov,
	pages = {107--112},
	publisher = {ACM},
	shorttitle = {{HBA} 1.0},
	title = {{HBA} 1.0: {A} {Pixel}-based {Annotated} {Dataset} for {Historical} {Book} {Analysis}},
	url = {https://dl.acm.org/doi/10.1145/3151509.3151528},
	urldate = {2023-08-20},
	year = {2017},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3151509.3151528},
	bdsk-url-2 = {https://doi.org/10.1145/3151509.3151528}}

@article{unsworth_medievalists_2012,
	author = {Unsworth, John},
	doi = {10.16995/dm.34},
	issn = {1715-0736},
	journal = {Digital Medievalist},
	month = feb,
	number = {0},
	title = {Medievalists as {Early} {Adopters} of {Information} {Technology}},
	url = {http://journal.digitalmedievalist.org/article/10.16995/dm.34/},
	urldate = {2023-08-20},
	volume = {7},
	year = {2012},
	bdsk-url-1 = {http://journal.digitalmedievalist.org/article/10.16995/dm.34/},
	bdsk-url-2 = {https://doi.org/10.16995/dm.34}}

@inproceedings{grana_picture_2009,
	address = {Santorini, Fira Greece},
	author = {Grana, Costantino and Borghesani, Daniele and Cucchiara, Rita},
	booktitle = {Proceedings of the {ACM} {International} {Conference} on {Image} and {Video} {Retrieval}},
	doi = {10.1145/1646396.1646426},
	isbn = {978-1-60558-480-5},
	language = {en},
	month = jul,
	pages = {1--8},
	publisher = {ACM},
	title = {Picture extraction from digitized historical manuscripts},
	url = {https://dl.acm.org/doi/10.1145/1646396.1646426},
	urldate = {2023-08-20},
	year = {2009},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/1646396.1646426},
	bdsk-url-2 = {https://doi.org/10.1145/1646396.1646426}}

@article{oliveira_dhsegment_2018,
	abstract = {In recent years there have been multiple successful attempts tackling document processing problems separately by designing task specific hand-tuned strategies. We argue that the diversity of historical document processing tasks prohibits to solve them one at a time and shows a need for designing generic approaches in order to handle the variability of historical series. In this paper, we address multiple tasks simultaneously such as page extraction, baseline extraction, layout analysis or multiple typologies of illustrations and photograph extraction. We propose an open-source implementation of a CNN-based pixel-wise predictor coupled with task dependent post-processing blocks. We show that a single CNN-architecture can be used across tasks with competitive results. Moreover most of the task-specific post-precessing steps can be decomposed in a small number of simple and standard reusable operations, adding to the flexibility of our approach.},
	author = {Oliveira, Sofia Ares and Seguin, Benoit and Kaplan, Frederic},
	copyright = {Creative Commons Attribution Share Alike 4.0 International},
	doi = {10.48550/ARXIV.1804.10371},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	note = {Publisher: arXiv Version Number: 2},
	shorttitle = {{dhSegment}},
	title = {{dhSegment}: {A} generic deep-learning approach for document segmentation},
	url = {https://arxiv.org/abs/1804.10371},
	urldate = {2023-08-20},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1804.10371},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1804.10371}}

@article{bell_nonverbal_2013,
	author = {Bell, Peter and Schlecht, Joseph and Ommer, Bj{\"o}rn},
	doi = {10.1080/01973762.2013.761111},
	issn = {0197-3762, 1477-2809},
	journal = {Visual Resources},
	language = {en},
	month = jun,
	number = {1-2},
	pages = {26--37},
	title = {Nonverbal {Communication} in {Medieval} {Illustrations} {Revisited} by {Computer} {Vision} and {Art} {History}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01973762.2013.761111},
	urldate = {2023-08-20},
	volume = {29},
	year = {2013},
	bdsk-url-1 = {http://www.tandfonline.com/doi/abs/10.1080/01973762.2013.761111},
	bdsk-url-2 = {https://doi.org/10.1080/01973762.2013.761111}}

@misc{noauthor_hba_2017,
	title = {{HBA} : un jeu d'images annot{\'e}es pour l'analyse de la structure de mise en page d'ouvrages anciens},
	url = {https://api.bnf.fr/fr/hba-un-jeu-dimages-annotees-pour-lanalyse-de-la-structure-de-mise-en-page-douvrages-anciens},
	year = {2017},
	bdsk-url-1 = {https://api.bnf.fr/fr/hba-un-jeu-dimages-annotees-pour-lanalyse-de-la-structure-de-mise-en-page-douvrages-anciens}}

@article{monnier_docextractor_2020,
	abstract = {We present docExtractor, a generic approach for extracting visual elements such as text lines or illustrations from historical documents without requiring any real data annotation. We demonstrate it provides high-quality performances as an off-the-shelf system across a wide variety of datasets and leads to results on par with state-of-the-art when fine-tuned. We argue that the performance obtained without fine-tuning on a specific dataset is critical for applications, in particular in digital humanities, and that the line-level page segmentation we address is the most relevant for a general purpose element extraction engine. We rely on a fast generator of rich synthetic documents and design a fully convolutional network, which we show to generalize better than a detection-based approach. Furthermore, we introduce a new public dataset dubbed IlluHisDoc dedicated to the fine evaluation of illustration segmentation in historical documents.},
	annote = {Other
Accepted at 2020 17th International Conference on Frontiers in Handwriting Recognition (ICFHR) (oral). Project webpage: http://imagine.enpc.fr/{\textasciitilde}monniert/docExtractor/},
	author = {Monnier, Tom and Aubry, Mathieu},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.2012.08191},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	note = {Publisher: arXiv Version Number: 1},
	shorttitle = {{docExtractor}},
	title = {{docExtractor}: {An} off-the-shelf historical document element extraction},
	url = {https://arxiv.org/abs/2012.08191},
	urldate = {2023-09-20},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2012.08191},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2012.08191}}

@misc{noauthor_computer_nodate,
	title = {Computer {Vision} and {Historical} analysis of {Scientific} illustration circulation ({VHS})},
	url = {https://vhs.hypotheses.org/},
	bdsk-url-1 = {https://vhs.hypotheses.org/}}
