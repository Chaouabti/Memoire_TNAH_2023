{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab90f7f3-427c-452b-bd48-200ddc34b8cc",
   "metadata": {},
   "source": [
    "# Data preparation and training\n",
    "\n",
    "These scripts are designed to prepare data for YOLOv8 training.\n",
    "\n",
    "The training folder must contain three elements :\n",
    "- a 'labels' folder: in which annotation files are stored,\n",
    "- an 'images' folder: in which image files are stored,\n",
    "- a 'labels.txt' file: containing annotation data in YOLO format: \n",
    "    - '0': 'class 0',/n '1': 'class 1',/n etc.\n",
    "    \n",
    "All these scripts are designed to process image and text data with the same name (except for the extension) contained in the 'labels' and 'images' folders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ee6c24-9f4b-41b4-b4a3-82bc16f5004a",
   "metadata": {},
   "source": [
    "## Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db34875-9c39-487a-a0a3-89508534379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from modules.class_names_functions import get_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11982cc-7b26-4d37-830c-0653252385a5",
   "metadata": {},
   "source": [
    "## Cleaning annotation files (.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097352de-5bcb-4f4e-a67c-3c9acbce10c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comma(dataset_folder):\n",
    "    \"\"\"\n",
    "    This function removes the commas that may appear when creating .txt files from a csv file.\n",
    "    \n",
    "    The 'dataset_folder' parameter is the path to the folder in which the dataset is stored.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(os.path.join(dataset_folder, 'labels')):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(dataset_folder, 'labels', filename)\n",
    "            # print(file_path)\n",
    "            \n",
    "            # Read the file content\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.read()\n",
    "            \n",
    "            # Remove commas\n",
    "            content_without_comma = content.replace(',', '')\n",
    "            \n",
    "            # Write the modified content in the file\n",
    "            with open(file_path, 'w') as file:\n",
    "                file.write(content_without_comma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8d8581-a974-496c-9c1b-6eee1f15dd38",
   "metadata": {},
   "source": [
    "## Create the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e51a4-c1dc-4f16-9eb7-d5744872c7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_training_dataset(dataset_folder, model_folder, newDistribution):\n",
    "\n",
    "    \"\"\"\n",
    "    This script prepares the data sets that will be used to train and validate the trained model.\n",
    "    This script generates 3 .txt files:\n",
    "    - A 'traindata.txt' file containing the list of images that will be used to train the model (80% of the data);\n",
    "    - A 'valdata.txt' file containing the list of images that will be used to evaluate the model (20% of the data);\n",
    "    - A 'training_dataset.txt' file containing the list of all the images used (training and validation).\n",
    "\n",
    "    N.B.: The creation of a \"test\" dataset has been removed because trained models are tested on similar data not contained in the training set.\n",
    "\n",
    "    The 'dataset_folder' parameter is the path to the folder in which the dataset is stored.\n",
    "    The 'model_folder' parameter is the path to the folder in which a pre-trained model data are stored\n",
    "    The 'distributionParameters' parameter default setting is True, and will create new .txt files to distribute the images used for training \n",
    "    and validation, as well as a .txt file with the entire training dataset.\n",
    "    If False, no file will be created and you can keep the pre-existing train/val distribution of the data.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    folder_base = os.path.dirname(dataset_folder)\n",
    "    dataset_name = os.path.basename(dataset_folder)\n",
    "    \n",
    "    # Folder in which all training-related files are stored\n",
    "    stat_folder = os.path.join(dataset_folder, 'dataset_statistics')\n",
    "    \n",
    "    if newDistribution == True:\n",
    "        # Get a list of the images\n",
    "        files = os.listdir(os.path.join(dataset_folder, 'images'))\n",
    "\n",
    "        # Filter file names to keep only those with \".jpg\" and \".png\" extensions\n",
    "        image_files = [f for f in files if f.endswith(\".jpg\") or f.endswith(\".png\")]\n",
    "\n",
    "        # Shuffle file names randomly\n",
    "        random.shuffle(image_files)\n",
    "\n",
    "        # Calcul le nombre d'images pour chaque ensemble\n",
    "        num_images = len(image_files)\n",
    "        num_train = int(num_images * 0.8)\n",
    "        num_val = int(num_images - num_train)\n",
    "\n",
    "        # Divide file names into two sets : one for the training, one for the validation\n",
    "        train_files = image_files[:num_train]\n",
    "        val_files = image_files[num_train:num_train+num_val]\n",
    "        \n",
    "        # Check if the destination folder exists, if not create it\n",
    "        \n",
    "        os.makedirs(stat_folder, exist_ok=True)\n",
    "\n",
    "        # Create a file with the list for the train data\n",
    "        with open(os.path.join(stat_folder, 'traindata.txt'), 'w') as f:\n",
    "            for image_file in train_files:\n",
    "                f.write(os.path.join(dataset_folder, 'images', image_file) + \"\\n\")\n",
    "        print(f\"File create in {os.path.join(stat_folder, 'traindata.txt')}\")\n",
    "\n",
    "        # Create a file with the list for valdidation data\n",
    "        with open(os.path.join(stat_folder,'valdata.txt'), 'w') as f:\n",
    "            for image_file in val_files:\n",
    "                f.write(os.path.join(dataset_folder, 'images', image_file) + \"\\n\")\n",
    "        print(f\"File create in {os.path.join(stat_folder, 'valdata.txt')}\")\n",
    "\n",
    "\n",
    "        # Create a file with all the dataset\n",
    "        with open(os.path.join(stat_folder, 'training_dataset.txt'), 'w') as f:\n",
    "            for image_file in image_files:\n",
    "                    f.write(os.path.join(dataset_folder, 'images', image_file) + \"\\n\")\n",
    "            print(f\"File create {os.path.join(stat_folder, 'training_dataset.txt')}\")\n",
    "    \n",
    "    else:\n",
    "        print(f'Use pre-existing files from {model_folder}.')\n",
    "        shutil.copyfile(os.path.join(model_folder, 'dataset_statistics/traindata.txt'), os.path.join(stat_folder,'traindata.txt'))\n",
    "        shutil.copyfile(os.path.join(model_folder, 'dataset_statistics/valdata.txt'), os.path.join(stat_folder,'valdata.txt'))\n",
    "        shutil.copyfile(os.path.join(model_folder, 'dataset_statistics/training_dataset.txt'), os.path.join(stat_folder,'training_dataset.txt')) \n",
    "    \n",
    "\n",
    "    # Split images and txt files into folders from a .txt file\n",
    "    split_data_for_training(os.path.join(stat_folder, 'traindata.txt'), \n",
    "                            os.path.join(dataset_folder, 'labels'), \n",
    "                            os.path.join(folder_base, 'datasets', dataset_name, 'images/train'), \n",
    "                            os.path.join(folder_base, 'datasets', dataset_name, 'labels/train'))\n",
    "    \n",
    "    split_data_for_training(os.path.join(stat_folder,'valdata.txt'),\n",
    "                            os.path.join(dataset_folder, 'labels'),\n",
    "                            os.path.join(folder_base, 'datasets', dataset_name, 'images/val'),\n",
    "                            os.path.join(folder_base, 'datasets', dataset_name, 'labels/val'))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f650486-2ca6-4a0e-94b7-a9a31b0508d1",
   "metadata": {},
   "source": [
    "## Split the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0b1d63-f392-4cca-ac3d-bb54184a9c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_data_for_training(txt_list, txt_folder, output_img_folder, output_txt_folder):\n",
    "    \n",
    "    \"\"\"\n",
    "    This script is used to divide, from the .txt files generated previously, the images and annotations into the folders \n",
    "    used by YOLOv8 for training and model validation. \n",
    "\n",
    "    YOLOv8's parameters require that the data be sent in a single folder, itself contained in the 'datasets' folder. Each training \n",
    "    folder must contain an 'images' folder and a 'labels' folder, each containing a 'train' folder and a 'val' folder.\n",
    "\n",
    "    The following script produces :\n",
    "    - A folder with the name of the training session\n",
    "    - In this folder :\n",
    "        - An images folder, containing\n",
    "            - a 'train' folder\n",
    "            - a 'val' folder\n",
    "        - A labels folder, containing\n",
    "            - a 'train' folder\n",
    "            - a 'val' folder\n",
    "    - The .txt files with the names of the image files used for training and their distribution in the 'train' and val sets,\n",
    "    as well as the .txt file with all the image files used for training.\n",
    "    \n",
    "    The parameters to be specified are :\n",
    "    \n",
    "    txt_list_train : path to txt file containing list of images for training\n",
    "    txt_list_val : path to txt file containing list of images for validation \n",
    "\n",
    "    output_img_folder_train : path to folder where images for training are to be stored\n",
    "    output_img_folder_val : path to folder where images for validation are to be stored\n",
    "\n",
    "    output_txt_folder_train : path to folder where annotations file for training are to be stored\n",
    "    output_txt_folder_val : path to folder where annotations file for validation are to be stored\n",
    "\n",
    "    According YOLOv8 documentation the folder must be contained in 'datasets' folder\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the output folder if it does not already exist\n",
    "    os.makedirs(output_img_folder, exist_ok=True)\n",
    "    os.makedirs(output_txt_folder, exist_ok=True)\n",
    "    \n",
    "    folder_base = os.path.dirname(dataset_folder)\n",
    "    dataset_name = os.path.basename(dataset_folder)\n",
    "    \n",
    "    # Open the text file containing the image paths\n",
    "    with open(txt_list, \"r\") as f:\n",
    "        # Browse through each line of the file\n",
    "        for line in f:\n",
    "            # Get the image path and text file name\n",
    "            image_path = line.strip()\n",
    "            image_name = os.path.basename(image_path)\n",
    "\n",
    "            txt_file = os.path.join(txt_folder, image_name).replace('.jpg', '.txt')\n",
    "            \n",
    "            # Copy image to output folder\n",
    "            shutil.move(image_path, os.path.join(output_img_folder, os.path.basename(image_path)))\n",
    "        \n",
    "            \n",
    "            # Copy text file to output folder\n",
    "            try:\n",
    "                shutil.move(txt_file, os.path.join(output_txt_folder, os.path.basename(txt_file)))\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                print(f'Text file {txt_file} does not exist')\n",
    "    print(f'Image files move in {output_img_folder}')\n",
    "    print(f'Text files move in {output_txt_folder}')\n",
    "    \n",
    "    #Create the yaml file\n",
    "    write_yaml_file(dataset_folder, dataset_name, folder_base)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7257e4d-d6ad-443d-9ff9-9587eb34c299",
   "metadata": {},
   "source": [
    "## Create the .yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c1055-68d4-4cdb-bd74-4c6006d07056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_yaml_file(dataset_folder, dataset_name, folder_base):\n",
    "    \n",
    "    '''\n",
    "    This function creates the .yaml file that will be used  to train the model.\n",
    "    \n",
    "    The 'datasets_folder' parameter is the path to the folder containing the training data.\n",
    "    The 'folder_name' parameter is the name of the training session. \n",
    "    The 'folder_base' is the path to the root folder.\n",
    "    '''\n",
    "    \n",
    "    # Get the annotations classes\n",
    "    annotation_classes = get_labels(os.path.join(dataset_folder, 'labels.txt'))\n",
    "    \n",
    "    # Convertir les clés du dictionnaire annotation_classes en entiers\n",
    "    annotation_classes_int = {int(key): value for key, value in annotation_classes.items()}\n",
    "\n",
    "    # Formater la chaîne avec les éléments dans l'ordre souhaité\n",
    "    yaml_data = f\"path: {os.path.join(folder_base, 'datasets', dataset_name)}/\\n\" \\\n",
    "                f\"train: 'images/train'\\n\" \\\n",
    "                f\"val: 'images/val'\\n\" \\\n",
    "                f\"\\n\" \\\n",
    "                f\"#class names\\n\" \\\n",
    "                f\"names:\\n\"\n",
    "    \n",
    "    for key, value in annotation_classes_int.items():\n",
    "        yaml_data += f\"  {key}: '{value}'\\n\"\n",
    "        \n",
    "    with open(os.path.join(folder_base, 'datasets', dataset_name, dataset_name + '.yaml'), 'w') as yaml_file:\n",
    "        yaml_file.write(yaml_data)\n",
    "    print(f\"File edit in {os.path.join(folder_base, 'datasets', dataset_name, dataset_name + '.yaml')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62ba4cd-693d-4a1a-8198-3d213c12916d",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d299c9f-8939-4dac-90a4-6142fac6b396",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def yolo_training(dataset_folder, use_model, img_size, epochs, batch, workers):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function allows you to start training by selecting and adjusting the model's performance, speed, and accuracy.\n",
    "    This list is not exhaustive.\n",
    "    For a complete list of modifiable arguments, \n",
    "    see :  https://docs.ultralytics.com/modes/train/#arguments.\n",
    "    \n",
    "    The 'datasets_folder' parameter is the path to the folder containing the training data.\n",
    "    \n",
    "\n",
    "    **Warning 1** : The arguments proposed \"by default\" are those that I have evaluated as being the most balanced \n",
    "    for training different models. They are suggestions rather than recommendations, and should be adapted to the data and \n",
    "    capacity of the equipment used.\n",
    "\n",
    "    **Warning 2** : The 'model_name' variable is constructed by taking the name of the training dataset, \n",
    "    the date and the parameters used for training.\n",
    "\n",
    "    The output folder in which the training data will be stored (save_dir) is constructed by YOLO in the form\n",
    "    **save_dir = project/name**. By default, project =  './runs/detect'.\n",
    "    \"\"\"\n",
    "    \n",
    "    folder_base = os.path.dirname(dataset_folder)\n",
    "    dataset_name = os.path.basename(dataset_folder)\n",
    "    \n",
    "    date = datetime.now().strftime('%Y%m%d')\n",
    "    model_name = f'{dataset_name}_{date}_{use_model[-4]}_i{img_size}_e{epochs}_b{batch}_w{workers}'\n",
    "    yaml_file = os.path.join(folder_base, 'datasets', dataset_name, dataset_name + '.yaml')\n",
    "\n",
    "    # Load a pretrained YOLO model\n",
    "    model = YOLO(use_model)\n",
    "\n",
    "    # Train the model\n",
    "    results = model.train(\n",
    "       data = yaml_file, #path to the datasets and classes\n",
    "       imgsz = img_size, #image size\n",
    "       epochs = epochs,\n",
    "       batch = batch,\n",
    "       workers = workers, # increases training speed, default setting is 8\n",
    "       name = model_name, # output folder\n",
    "       project = os.path.join(folder_base, 'runs/train')\n",
    "    )\n",
    "\n",
    "    # Evaluate the model's performance on the validation set\n",
    "    results = model.val(\n",
    "        name = model_name + '/'+ model_name +'_val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f36fa-fd39-4dc6-9229-aa6c4037aaae",
   "metadata": {},
   "source": [
    "### Resuming interrupted trainings(Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ad3b74-b5f5-4e59-825b-aba110229f19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resume_training(model_folder):\n",
    "    \n",
    "    '''\n",
    "    This function resumes an interrupted training session.\n",
    "    The resume parameter retrieves all previously declared parameters\n",
    "    The parameters are :\n",
    "    - 'last_weight': path to last trained weight ('last.pt'),\n",
    "    - 'model_name': used for the val session.\n",
    "    '''\n",
    "    \n",
    "    last_weight = os.path.join(model_folder, 'weights/last.pt')\n",
    "    model_name = os.path.basename(model_folder)\n",
    "    \n",
    "    # Load a model\n",
    "    model = YOLO(last_weight)  # load a partially trained model\n",
    "\n",
    "    # Resume training\n",
    "    results = model.train(resume=True)\n",
    "\n",
    "    # Evaluate the model's performance on the validation set\n",
    "    results = model.val(\n",
    "        name = model_name + '/'+ model_name +'_val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0ab296-0f02-4332-87d6-8c1d7c8ed3b1",
   "metadata": {},
   "source": [
    "## Re-arrange in pristine state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afb066e-962c-41bf-a3ba-bc8e4bb4b802",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dispatch_data(dataset_folder, model_folder):\n",
    "\n",
    "    \"\"\"\n",
    "    This function moves the .txt file containing the distribution list of data used for training, and the .yaml file, \n",
    "    into the model folder and returns the image data and annotations to the original folder.\n",
    "    Finally, the folder used for training is deleted.\n",
    "\n",
    "    In this way, a new model can be trained by reusing this notebook, and each trained model will have a specific folder\n",
    "    with all the data linked to it.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    folder_base = os.path.dirname(dataset_folder)\n",
    "    dataset_name = os.path.basename(dataset_folder)\n",
    "    \n",
    "    date = datetime.now().strftime('%Y%m%d') \n",
    "    model_name = f'{dataset_name}_{date}_{use_model[-4]}_i{img_size}_e{epochs}_b{batch}_w{workers}'\n",
    "\n",
    "    \n",
    "    if model_folder == '':\n",
    "        model_folder = os.path.join(folder_base, 'runs/train', model_name)\n",
    "    else:\n",
    "        model_folder = model_folder\n",
    "    \n",
    "    # Move the data used for the training session into the model folder\n",
    "    \n",
    "    shutil.move(os.path.join(dataset_folder, 'dataset_statistics'), model_folder)\n",
    "    print(f'The .txt files with the training data have been moved to {model_folder}.')\n",
    "  \n",
    "    shutil.move(os.path.join(folder_base, 'datasets', dataset_name, dataset_name + '.yaml'), os.path.join(model_folder, dataset_name + '.yaml'))\n",
    "    print(f'The .yaml file has been moved into {model_folder}')\n",
    "    \n",
    "    shutil.copyfile(os.path.join(dataset_folder, 'labels.txt'), os.path.join(model_folder, 'labels.txt'))\n",
    "    print(f'The labels.txt file has been copied in {model_folder}')\n",
    "\n",
    "    # Replace the data in the dataset folder\n",
    "    img_folder_train = os.path.join(folder_base, 'datasets', dataset_name, 'images/train') \n",
    "    txt_folder_train = os.path.join(folder_base, 'datasets', dataset_name, 'labels/train')\n",
    "    img_folder_val = os.path.join(folder_base, 'datasets', dataset_name, 'images/val')\n",
    "    txt_folder_val = os.path.join(folder_base, 'datasets', dataset_name, 'labels/val')\n",
    "    \n",
    "    for file in os.listdir(img_folder_train):\n",
    "        shutil.move(os.path.join(img_folder_train, file), os.path.join(dataset_folder, 'images', file))\n",
    "    print(f\"Files from {img_folder_train} have been moved into {os.path.join(dataset_folder, 'images')}\")\n",
    "    \n",
    "    for file in os.listdir(img_folder_val):\n",
    "        shutil.move(os.path.join(img_folder_val, file), os.path.join(dataset_folder, 'images', file))\n",
    "    print(f\"Files from {img_folder_val} move into {os.path.join(dataset_folder, 'images')}\")\n",
    "    \n",
    "    for file in os.listdir(txt_folder_train):\n",
    "        shutil.move(os.path.join(txt_folder_train, file), os.path.join(dataset_folder, 'labels', file))\n",
    "    print(f\"Files from {txt_folder_train} move into {os.path.join(dataset_folder, 'labels')}\")\n",
    "    \n",
    "    for file in os.listdir(txt_folder_val):\n",
    "        shutil.move(os.path.join(txt_folder_val, file), os.path.join(dataset_folder, 'labels', file))\n",
    "    print(f\"Files from {txt_folder_val} move into {os.path.join(dataset_folder, 'labels')}\")\n",
    "\n",
    "    shutil.rmtree(os.path.join(folder_base, 'datasets', dataset_name))\n",
    "    print(f\"The {os.path.join(folder_base, 'datasets', dataset_name)} has been deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46759553-9332-4938-af01-3a8a0c179a33",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f5197c-5312-4a46-8974-f24cf2a3efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = 'ABSPATHTODATASET' # to be changed, absolute path to the dataset you will use for the training session\n",
    "\n",
    "# if you don't want to use pre-existing files or if this is the first training session model_folder = ''\n",
    "model_folder = '' # to be changed, absolute path to the model folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fa1ca5-16a8-4341-b643-89e6535f3d7c",
   "metadata": {},
   "source": [
    "### Clean and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ad5c3b-9e17-42d4-8313-b7e69e2c1056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the file txt if needed\n",
    "clean_comma(dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e7bf7-9f5f-4745-a764-4d7999c26969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data distribution file for train/val sets\n",
    "create_training_dataset(dataset_folder,model_folder, newDistribution=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faef8d5-8d19-4d9a-9663-b3006efd81c8",
   "metadata": {},
   "source": [
    "### Start a training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26d4889-bda8-4246-9ffe-433fca7f097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_model = 'yolov8l.pt' # to be changed as needed, by default use 'yolov8x.pt'\n",
    "img_size = 640 # to be changed as needed, by default use 640\n",
    "epochs = 100 # to be changed as needed, by default use 100\n",
    "batch = 8 # to be changed as needed, by default use 8\n",
    "workers = 24 # to be changed as needed, by default use 24 or -1 for AutoBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd918a0-d728-4bf6-bb3f-4d3d9cbede6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Start a training session\n",
    "yolo_training(dataset_folder, use_model, img_size, epochs, batch, workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74460189-6de3-452a-8713-ae5952dabf0a",
   "metadata": {},
   "source": [
    "### Dispatch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aae5b7b-d9ce-408c-b677-c8d2496ff2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the .txt files describing the distribution of images/labels in train and val of the training data into the model folder and replace the image/label data themself in their original folders\n",
    "dispatch_data(dataset_folder, model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfa8732-333b-4265-a1e0-a661535e6ceb",
   "metadata": {},
   "source": [
    "### Resume an uncompleted training session (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c0586a-701b-4d32-9379-8ec8c18906db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "# Resume an interrupted training\n",
    "# resume_training(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfba586e-bfdc-47d0-aedd-e88f94560e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the .txt files describing the distribution of images/labels in train and val of the training data into the model folder and replace the image/label data themself in their original folders\n",
    "# dispatch_data(dataset_folder, model_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
